## Multinomial Logistic Regression(다중 로지스틱 회귀)
feature(독립변수)에 의해 나오게 되는 결과값인 Target(종속변수)가 0 or 1과 같은 binary 형태로 나오는 것이 아니라 A/B/C와 같이 여러 범주가 결과로 나오는 회귀분석을 의미함

이러한 다중분류는 관점을 다르게 보면 여러 개의 binary classification이라고도 볼 수 있음
1. A 또는 A가 아닌 것
2. B 또는 B가 아닌 것
3. C 또는 C가 아닌 것

이런 식으로 볼 수 있음
각 분류를 Class라고 부름

그래서 Multinomial Logistic의 경우 위의 3가지 경우에서 A인 확률/B인 확률/C인 확률들을 모두 추출하고 이것을 합산한 것을 분모로 하여 각 case(A, B, C)가 나올 확률을 각각 다시 계산한다.

##### one-hot Encoding : 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 표현방식(여기서는 A가 예측값이다 싶으면 A는 1 나머지 B, C는 0이라고 표현하는 것)

계산하고 나면 one-hot Encoding이라고 하는 방식을 통해 예측값과 정답데이터의 형태를 변화하는 과정을 거치고 비교를 통해 차이를 줄여가는 방식으로 학습을 하게 됨

여기서 비교에 사용되는 수식이 바로 categorical_crossentropy임(이전에는 binary_crossentropy를 썼었음)

##### output Layer에서
Class의 갯수가 늘어날수록 W와 b에 대한 설정도 늘어나게 된다.
Class의 갯수에 따라 동그라미의 갯수가 결정된다.

## Softmax 함수
2개의 범주를 분류하는 함수로 **Sigmoid**를 사용했다면, 3개 이상의 여러 범주를 분류할때 사용하는 함수가 바로 **Softmax**함수이다.


## 이미지 처리
비정형 데이터의 처리
**MNIST** : 이미지 데이터을 이용한 다중 분류를 말함
여기서의 이미지 데이터란 사람이 손글씨로 숫자를 쓴 이미지를 의미한다.

Color 이미지의 경우, RGB를 통해 3개의 값을 가지고 색을 표현한다.
이 Color 이미지를 데이터로 변경할 경우 3차원 데이터로 표현이 된다.

흑백 이미지의 경우, 색을 0과1 중 한 개의 값을 가지고 표현할 수는 있지만 많이 사용하지는 않는다!

반면에 한 개의 값을 사용하되 범위를 0~255 로 잡아서 표현할 수도 있다. 그 때 나오게 되는 것이 `gray scale` 이다.

머신러닝의 첫 번째는 형태를 파악한다.  
이미지가 사람인지, 자동차인지, 고양이인지, 강아지인지.  
이 때는 굳이 컬러 데이터가 아니어도 흑백 데이터를 가지고도 구분할 수 있기 때문에 흑백 데이터를 사용한다.

머신러닝은 행렬곱을 사용해야 하기 때문에 반드시 2차원 데이터가 필요했다.
그런데 이미지의 경우 컬러데이터는 3차원이고, 여러 개의 이미지가 사용되다보니 4차원이 되었다.

그래서 이미지를 흑백 데이터로 바꿔서 2차원으로 했는데 여러 개의 이미지로 층을 쌓다보니 3차원이 되었다.
최종적으로 이미지를 1차원 데이터로 바꾸니 드디어 여러 장의 이미지로 행을 쌓더라도 2차원 데이터로 행렬곱이 사용할 수 있게 되었다.

### 과적합(Overfitting)

epochs, 즉 반복의 횟수가 많아질수록 기준을 잡게 되는 구분선이 너무 과도하게 정밀해진다. 그러면 기준선을 기준으로 `0/1` 이라는 binary 구분을 해야 하는데 학습된 값을 기준으로 구분을 하게 되어 버린다.

결론적으로 비정형 데이터는 일반 머신러닝 기법을 이용하면 학습에 효율(정확도)이 많이 떨어진다.
이를 해결할 방법이 뭐가 있을지 찾다가 발견한 것이 Hidden Layer

Hidden layer 내부의 수많은 unit(node)을 통해 많은 경우의 수(가중치)를 한번에 계산하게 하여 한번 loss를 계산하여 갱신할 때 여러 경우의 수와 비교해볼 수 있도록 함
hidden layer 는 1개가 들어갔을 때 효율이 가장 좋다.
2개가 들어갈 경우, 1개 넣었을 때보다 0.7배 만큼 더 정확해진다고 한다. 

Hidden layer의 경우 unit을 임의로 아무렇게나 줄 수 있음(몇 개가 존재하도록 해야 효율적인지는 안 배움)
근데 여기서 과적합 문제가 발생함 (쉽게 말해 학습 데이터에 대한 고착화가 일어난 것) 

이것을 dropout이라는 것을 통해 해결!
임의로 정해준 unit의 수 중 몇 퍼센트만 사용할 지 결정해줄 수 있음

#### Dropout Layer
과적합을 방지하기 위해 새로운 layer 를 추가  
dropout 이라는 방법을 통해서 연산을 할 때 모든 레이어의 연산을 하는 것이 아니라,  
일부의 연산을 제거하여 과적합을 방지하도록 진행한 것

param의 갯수는 즉 w와 b. param 1개 당 한 번의 편미분을 하게 되기 때문에 약 24만 개의 param 에 대한 편미분을 진행하게 된다.
때문에 너무나 많은 param 값이 나온다면, 그것을 조절해주어야 한다.

model 의 학습 과정을 그래프로 확인하는 방법
