### 인공지능을 구현하는 두 가지 방법
Top-down : 이데아가 인간에게 지식을 줬듯이 프로그래머(인간)가 컴퓨터에게 지식을 주는 방식(흔히 알고 있는 rule 기반의 프로그램)
Bottom-Up : 컴퓨터가 경험을 바탕으로 지식을 직접 학습하게 하는 방식(머신러닝이라고 부름 -> 현재의 딥러닝(신경망인데 신경망의 깊이가 매우 깊은 신경망을 말함))

즉, 머신러닝은 인공지능을 구현하는 방법 중 하나임

### 신경망 - 신경망은 인간의 뇌를 모방하여 만든 수학적 모델

머신러닝 과정의 구성요소
1. 추론
2. 추론 평가(분류-우도(likelhood)/회귀-에러의 제곱(평균제곱오차)) 3. 최적화(잘 추론하는 방향으로 모델을 업데이트하는 것)

### 하기 전에
- 배치사이즈 : 전체 데이터에서 쪼개진 데이터 뭉치의 갯수
- bias(편향) : 특정 임계점이 1이라고 했을 때 편향이 0.5라면 출력값이 0.5만 되어도 임계점이 도달하는 것과 같이 임계점에 얼마나 쉽게 도달하게 할 것이냐를 정해줄 수 있는 숫자(가중치 외에 결과에 영향을 미칠 수 있는 요소를 고려해서 넣는 숫자이기도 함)
- 1epoch은 모든 데이터를 한번씩 다 훑었을 때를 말한다.(배치사이즈가 모두 돌았을 때 1epoch)
- 일반적으로 2차원(표의 형태)으로 input값들이 들어온다고 했을 때 행 단위로 feature(독립변수)마다 가중치가 적용된 후 비선형 변환 과정(여기서 활성화 함수가 사용됨)이 모든 행에서 다 이뤄졌을 때 loss function을 이용하여 오차제곱의 합을 구하고 그 합으로 평균(평균제곱오차)를 구하게 됨

(`학습할 때마다 변해가는 평균제곱오차와 가중치의 변화를 그래프로 그린 것이 손실함수인 것)
--> 근데 여기서 행은 가로줄 모양이고  node(각 input에 대하여 가중치가 적용되는 칸들)은 세로줄 형태로 늘어서 있기 때문에 (행 데이터 x 가중치)는 행렬곱 연산으로 이뤄지게 되는 것임!

- 단, dataSet이 너무 크기가 클 경우에는 미니배치(mini batch)라고 하여 작은 단위로 dataSet을 쪼개서 학습하게 됨  --> 한 미니배치 단위가 학습 할 때마다 가중치를 조정하게 됨(물론 1epoch으로 가중치를 조정하게끔 설정할 수도 있음)


**loss function을 통해 계산된 평균제곱오차(loss)와 가중치 변화에 대한 그래프**
![](../../README_resources/Pasted%20image%2020230706102053.png)
- 가로축이 가중치, 세로축이 오차(에러)를 나타내는 것
- 임의로 가중치를 계속 늘리거나 줄였을 때의 변화를 함수로 나타내면 이런 모양일 것임
- 가중치마다 이런 함수가 그려질 것이고 한 학습마다 각각의 가중치에 대해서 편미분을 진행하여 조정값을 구하여 조정하게 됨

- 물론 미분값이 0이 되는 지점이 가장 오차가 적은 지점이 될 것임
- 그래서 loss function을 통해 나온 결과값을 미분한 값이 0이 아니라면 학습률(어느 정도나 조정할 것인가에 대한 숫자)을 이용하여 가중치를 조절하게 됨 

실제 가중치 조절 공식(얘가 GD라는 옵티마이저)
![](../../README_resources/Pasted%20image%2020230706103659.png)
- 해당 지점에서의 기울기 = 미분값
- 학습률은 얼마나 큰 보폭으로 가중치를 조정해 나갈지의 대한 숫자
- 막상 공식에 나와 있는 해당 지점에서의 기울기에 위치한 부분에 대입되는 놈은 다시 구해야 하는 수식의 형태임 -> 그럼 얘는 또 어케 구함? 바로 아래를 참고! 
- 편미분과 연쇄 법칙이 사용됨(한번에 loss에 대한 weight의 미분값을 구할 수 없기 때문에 연쇄법칙 사용)
- 아마 loss의 결과를 나타내는 수식을 전개해서 미분하기에는 너무 복잡한 경우가 많아서 연쇄법칙이 사용되는 듯함
![](../../README_resources/Pasted%20image%2020230706184404.png)
- Z : 이전층의 모든 입력이 각각의 가중치와 곱해진 값들이 모두 더해진 가중합
1. 변수 갯수만큼 각각 편미분함
2. 연쇄 법칙을 활용한 결과 도출(밑에 있는 것이 연쇄 법칙 과정 - 합성함수의 미분)
![](../../README_resources/Pasted%20image%2020230706184519.png)
- w(weight)에 대한 미분값을 구하기 위해서는 weight라는 변수가 포함되어 있는 함수가 가장 안쪽에 위치하고 있기 때문에, 차근차근 바깥 함수부터 미분하여 곱하기를 해야 함(흔히 알고 있는 합성함수 미분 공식처럼) => 그래야 위 식처럼 약분으로 다 지워지고 loss에 대한 weight의 미분값을 구할 수  있게 됨
- 여기서 편미분은 `각 가중치에 대해서 ∂Z/∂w`를 하거나  `∂Z/∂b`를 할 때 사용될 것임

- 가중치 조절 공식을 통해 실제 조정할 값을 정하여 갱신이 이뤄진다.
- 이런 식으로 가중치 조절이 이뤄지는 게 바로 경사하강법(GD : Gradient Descent)

### 옵티마이저 - 경사하강법(GD) 너무 비효율적 ㅋ 해서 나온 알고리즘
SGD - 한 학습단위에 대해서 하나의 데이터만 가지고 학습 but 정확도 떨어짐 

- **활성화 함수로 비선형 변환하는 이유** : 입력값에 대한 정답의 분포가 있다고 할 때 이를 직선이 아닌 곡선을 통해 나타내면 데이터의 분포를 보다 정교하게 나타낼 수 있음 -> 이 곡선이 바로 우리가 만들려는 모델 즉, 수식임
Ex) 가중치가 2라고 했을 때 입력값(input)에 대해서 hidden layer가 아무리 많아도 inputx2x2...와 같이 결국에는 직선으로밖에 표현할 수 없어서 아무리 학습해도 제대로 된 예측을 할 수 없었음(제대로 학습이 안 된다고 할 수 있는 거지) 그래서 생각해낸 방법이 비선형 변환인 것
=> 쉽게 말해서 직선을 휘게 만들어서 정답의 분포를 더 정교하게 만들자

- 활성화함수는 비선형 변환을 할 때만 사용하는 것이 아님 / 분류 문제 같은 것에 시그모이드나 소프트맥스 같은 활성화 함수로 출력값에 대해서 실제 확률이나 class값으로 변환하여 예측치로 내보내는 역할도 수행함 
- 또한 스케일 차이 많이 나는 결과값들을 비슷한 크기로 정규화해주는 역할도 수행함
(시그모이드 함수로 모든 값을 0~1사이의 값으로 만들어주는 걸 예로 들 수 있음)








