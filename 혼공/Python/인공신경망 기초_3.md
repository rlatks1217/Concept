### 하이퍼 파라미터
1. 학습률(learning rate): 가중치 갱신 시 사용되는 스케일 조정 값으로, 학습 속도에 영향
2. 배치 사이즈(batch size): 한 번의 학습에 사용되는 샘플의 수, 메모리 사용량과 학습 속도에 영향
3. 에포크(epoch): 전체 데이터셋을 몇 번 반복하여 학습할지 결정합니다.
4. 정규화(regularization): 과적합을 방지하기 위해 가중치를 제한하는 방법
5. 은닉층의 수와 크기: 신경망 모델에서 은닉층의 수와 각 은닉층의 뉴런 수를 결정
6. 드롭아웃(dropout): 학습 중에 일부 뉴런을 무작위로 비활성화하여 과적합을 줄이는 방법(한 번 학습할 때 모든 가중치를 업데이트하는 것이 아니라 일부 가중치만 업데이트하게 되는 것임)

### 과적합(overfitting)
- 너무 학습 데이터셋에만 적합하게 학습하여 실제로 예측을 잘 못하게 되는 것

## CNN : https://seahahn.tistory.com/category/Deep%20Learning/Computer%20Vision

### 합성곱 신경망(CNN; Convolutional Neural Network)
- 합성곱 신경망은 시각적 영상을 분석하는 데 사용되는 인공신경망의 한 종류
- 딥 러닝에서 심층 신경망으로 분류되며, 시각적 영상 분석에 주로 적용됨
- 영상(사진) 및 동영상 인식, 추천 시스템, 영상 분류, 의료 영상 분석 및 자연어 처리 등에 응용됨

**합성곱** : 간단하게 말해서 짝이 맞는 두 숫자들끼리 곱하고 곱한 결과들끼리 모두 더하는 것을 말함

![](../../README_resources/Pasted%20image%2020230718154857.png)

- 과정
1. 입력 데이터가 들어면 특징 추출 부분에서 합성곱 연산 및 풀링 등의 과정을 거쳐 이미지의 특징을 잡.아낸다.
2. 분류기에서 어떤 이미지인지 판단한다.

![](../../README_resources/Pasted%20image%2020230718155132.png)

- 5X5 짜리 흑백(그레이 스케일) 이미지가 input으로 들어온다고 할 때 2X2짜리 필터가 왼쪽 위부터 시작해서 사용자가 설정해준 칸(strides로 설정) 단위로 돌아다닌다.
- 한 칸씩 움직인다고 가정했을 때, 필터는 가로 4번, 세로 4번 해서 총 16번 움직일 것이다.
- 이 때 필터의 각 칸들과 맞닿는 부분들이 곱해져서 합산이 됨(여기서 맞닿는 부분들을 짝이 맞는 애들이라고 표현한 것) 
- 이렇게 해서 결과로 나온 것들을 4X4형태(가로, 세로 4번씩 합성곱 + 합산의 과정이 있었을 테니까 4X4형태임)로 나열해놓은 것을 Feature Map이라고 한다.(특성 추출)

- 그레이 스케일(channel(이미지를 나타내는 독립된 데이터 평면)이 한 개)의 경우 필터가 곧 커널이다
- 하지만 컬러 이미지(RGB -> channel이 3개)의 경우 하나의 필터 안에 3개의 커널이 있다
- 즉, 각 채널마다 커널이 있고 필터는 커널의 집합이라고 할 수 있다.

컬러 이미지의 경우 3가지 채널에 대하여 특성을 모두 뽑은 후에 각 채널에 대한 특성들을 처음에 각 커널들을 통해 특성을 뽑을 때처럼 맞닿는 부분들을 더하여 하나로 만든 다음 출력함

**Stride**
- 걸음, 보폭이라는 의미를 가지고 있음
- 실제로 코드에서 이 strides라는 매개변수에 특정 값을 넣어줌으로써 몇 칸 단위로 돌아다닐지 정할 수 있음

**Padding**
- 이미지 외부에 패딩을 줄 수 있음
- 패딩은 필터가 돌아다닐 때 외곽 부분은 겹치는 부분이 많지 않아 계산되는 경우가 안 쪽보다 적음. 그것을 약간 보완하고자 패딩을 줘서 겹치는 횟수를 늘릴려는 의도로 주기도 함
- 또한 패딩에 의해 필터가 돌아다니는 전체 영역이 커지면서 Feature Map도 자연스럽게 커짐 => 이를 이용하여 원래 이미지 데이터와 동일한 shape의 Feature Map을 만들 수도 있음(이 Feature Map의 크기를 무슨 이유로 조절하는지는 잘 모름)
- 패딩의 값은 보통 안 주는 편인데 이를 제로 패딩(Zero Padding)이라고 함


**Pooling**
- Feature Map을 풀링과정을 통해 정리된 결과로 얻을 수 있다
- 그림 보면서 이해

![](../../README_resources/Pasted%20image%2020230718213019.png)

- 그림과 같이 일정한 간격으로 나눈 다음 그 영역의 특징을 가장 잘 설명하는 값을 가져옴
- 여기서 영역의 특징을 가장 잘 설명하는 값을 최대값으로 가져올지 평균값으로 가져올지에 따라 Max Pooling과 Average Pooling으로 구분된다.(주로 Max Pooling이 사용됨)
- 코드로 작성할 때는 Pooling할 영역을 (행, 열)로 사이즈를 지정하여 사용하게 됨
- 풀링을 하게 되면 그림에서 보이는 것처럼 실제로 메모리에서 차지하는 정도로 작아져서 더 효율적이라고 할 수 있음 / 동시에 파라미터(여기서 파라미터는 가중치(변수)를 의미함)도 줄어들게 되어 과적합을 방지하는 효과도 있음(이것은 큰 필터 대신 작은 필터로 여러 층에 걸쳐 쓰는 것이 좋은 장점이기도 하다)

**Colaboratory 코드 참고**

### 전이학습(Transfer Learning)

- 일반적으로 전이 학습은 대량의 데이터를 학습해놓은 사전 학습 모델(Pre-trained Model)의 가중치를 그대로 가져온 후, 분류기(완전 연결 신경망이라고 함 - 수치 계산해서 예측치 만들어 내는 부분)만 필요하면 추가적으로 붙여서 설계하여 모델을 만드는 것이라고 할 수 있음

**특징**
- 미리 대량의 데이터를 통해 학습한 모델의 가중치를 그대로 가져온 것이므로 일반적인 아무 데이터를 입력값으로 넣어도 준수한 성능을 보임
- 사전 학습 가중치가 되도록 학습하는 것이 아니라 해당 가중치를 모델에 고정해놓고 시작하는 것이기 때문에 빠르게 좋은 결과를 얻을 수 있음
- 학습 데이터가 적거나 전이학습 없이 학습할 때보다 더 정확한 편임

flatten() 사용 이유 : 일부 딥러닝 모델은 이러한 다차원 입력을 받지 못하고 1D 배열로만 처리할 수 있음이 경우, Flatten 작업을 사용하여 다차원 배열을 1D 배열로 펼치고 모델에 입력으로 전달할 수 있음





