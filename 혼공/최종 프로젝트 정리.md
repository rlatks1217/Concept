console.log에 찍힌 Array를 실제로 열어보는 시점에는 그 시점의 Array 상태를 보고 가져와서 console에 보여주게 된다. 즉, log 코드를 splice 같은 것으로 삭제하기 전에 실행했더라도 여는 것은 삭제되고 나서 열게 되므로 열어보는 시점엔 삭제된 이후의 상태를 가져와서 보여주게 되는 것이다.

## 문장 요약 알고리즘

PlaintextParser.from_string(text, Tokenizer("korean")) : 한국어 문장의 분리 기준으로 들어오는 text를 문장 단위로 나누고 그 나눠진 문장들을 가진 객체를 생성해주는 코드

tokenizer : 문장분리 기준을 적어주는 속성임 한국어 문장을 분리하기 위해 korean을 속성값으로 적는다.(english라고 적지 않는 이유는 영어 문장은 공백을 기준으로만 문장을 분리하면 되기 때문에 english라고 적어도 상관없지만 한국어 문장은 '마침표(.)', '물음표(?),'느낌표(!)', '따옴표(')', '괄호' 등의 기호를 사용하여 보다 정교하게 분리해야 하기 때문에 korean이라고 적는 것이 적절하다.)

summarizer = LsaSummarizer() : parser.doucument로 문장들을 받아서 그 문장들의 요약본을 생성하는 역할을 하는 객체
summary = summarizer(parser.document, 1) : 해당 객체를 통해서 1문장으로 요약해주고 요약본을 tuple형태로 반환해주는 코드 괄호 안에는(요약할 문장을 담은 객체, 몇개의 문장으로 요약할지)를 적어준다.


## 유사도 측정 알고리즘

### 벡터화
Term Frequency : 한 문서(문장) 내에서 단어가 등장한 횟수를 나타내는 지표임 즉, 해당 단어의 등장 빈도를 의미함 => 한 문서에서 등장 횟수가 많다면 중요한 역할을 한다고 판단함, 그런데 the, of와 같은 불용어는 등장 빈도수가 높다고는 해도 실제로 중요한 의미를 담고 있는 단어는 아니기 때문에 제거하고 계산해야 함(TfidfVectorizer를 사용할 땐 내부적으로 알아서 계산 전에 불용어를 제거하는 과정을 거침)

ex) 문서 A에서 단어 "apple"이 3번 등장했다면, "apple"의 TF 값은 3
Inverse Document Frequency (IDF): 전체문서에서 해당 단어가 등장한 문서의 갯수를 의미함 해당 단어가 많은 문서에서 등장한다는 것은 그만큼 흔하게 사용되는 단어이고 그것은 곧 그렇게 중요한 역할을 하는 단어가 아닌 흔한 단어일 뿐이라고 판단할 수 있음

##### 즉, Term Frequency는 높으면서 Inverse Document Frequency (IDF)는 낮은 단어가 중요한 역할을 하는(핵심을 가리킬 가능성이 높은) 단어라고 할 수 있음
##### 그래서 벡터화를 한 결과에서 각 수치가 의미하는 바는 결국 단어의 중요도임

### 벡터화 과정
1. 문장을 토큰화(문장을 여러 단어로 나누는 과정)한다. 이 과정에서 문장부호나 불용어 등은 제거한다.
2. 각 단어의 TF-IDF 값을 계산한다. 이 값은 해당 단어가 문장에서 얼마나 중요한지를 나타낸다
3. 문장마다 벡터화를 거쳐 결과를 각각 벡터 형태(1차원 list 형태)로 저장하게 된다.
4. 여러 문장을 벡터화하기 때문에 전체 list안에 벡터화가 된 결과들이 list형태로 들어가게 되는 것이다(즉, 결과가 2차원 list 형태가 됨)

### 유사도 측정 과정
1. 모든 문장에서 공통적으로 나타내는 단어를 불용어로 보고 제거 후 벡터화를 하기 위해서 모든 문장을 담은 list를 vectorizer 안에 넣는다.
2. 벡터화 후 그 list에서 두 문장의 유사도를 계산한 후 결과를 반환하게 된다. Matrox 형태로 반환하게 된다.
3. 유사도를 측정할 문장들만 벡터화시키는 것이 가장 정확하다.

### 유클리드 거리 유사도
유클리드 거리는 단어의 빈도수가 유사하지 않은 경우에 정확한 결과를 얻을 수 없음
ex) "I love cats and dogs"가 많이 등장하는 A문서와 "I love turtle"가 많이 등장하는 B문서가 있다고 했을 때 "I love"두 문서에 공통적으로 많이 등장한다는 점에서 두 문서가 서로 유사한 것으로 판단해야 함 => 하지만 유클리드 거리 유사도의 경우 거리를 기준으로 유사도를 측정하기 때문에 다르다고 판단할 가능성이 큼

또한 비교하려는 문장 간의 스케일(벡터의 크기(길이))이 많이 차이 나는 경우에는 정확한 측정이 어려울 수 있기 때문에 사용하는 것은 좋지 않음 => 반면, 코사인 유사도는 두 지점의 방향이 같으면 유사하다고 판단하기 때문에 스케일이 차이나더라도 상대적으로 정확한 유사도 측정이 가능함


### 두 문장이 관련없는 경우에도 유사도가 높은 이유
단어의 빈도수가 유사하게 분포하는 경우: 이 경우, 두 비교대상들이 의미상으로는 연관이 없을 수 있지만, 단어 사용 패턴이 유사하여 유사도가 높게 나타날 수 있습니다.
문장 자체가 길지 않기 때문에 핵심 단어라도 단어의 빈도수가 크지 않아 핵심 단어를 기준으로 유사도를 측정하기에 어려움

1. vs Code에 python 파일 만들기
2. 실행하려는 코드 작성
3. python 파일명.py로 서버 기동
4. pip install flask-restful 설치
5. CORS 해결을 위해 pip install flask-cors 설치 후 import


### TF - IDF
하나의 Document에서 DF는 1임(100개의 문서라면 DF = 100)
100개의 문서에서 특정 단어가 10개 나왔다면 IDF값은 10(100/10)임
모든 문서에서 자주 등장하는 단어의 중요도는 낮고 특정 문서에서 자주 등장하는 단어는 중요도가 높다고 판단

사용자가 검색을 했을 때 어떤 데이터가 먼저 보여질지 판단할 때 중요한 척도가 됨

1. 토큰화
2. 불용어 제거
3. 정규화(단어를 원형으로 만듬/정규화를 거치지 않으면 각기 다른 같은 의미의 단어들의 빈도수가 모두 개별적으로 counting될 것임)