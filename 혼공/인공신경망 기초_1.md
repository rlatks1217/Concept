### 인공지능을 구현하는 두 가지 방법
Top-down : 이데아가 인간에게 지식을 줬듯이 프로그래머(인간)가 컴퓨터에게 지식을 주는 방식(흔히 알고 있는 rule 기반의 프로그램)
Bottom-Up : 컴퓨터가 경험을 바탕으로 지식을 직접 학습하게 하는 방식(머신러닝이라고 부름 -> 현재의 딥러닝(신경망인데 신경망의 깊이가 매우 깊은 신경망을 말함))

즉, 머신러닝은 인공지능을 구현하는 방법 중 하나임

### 신경망 - 신경망은 인간의 뇌를 모방하여 만든 수학적 모델

머신러닝 과정의 구성요소
1. 추론
2. 추론 평가(분류-우도(likelhood)/회귀-에러의 제곱(평균제곱오차)) 3. 최적화(잘 추론하는 방향으로 모델을 업데이트하는 것)

### 하기 전에
- 1epoch은 모든 데이터를 한번씩 다 훑었을 때를 말한다.
- 일반적으로 2차원(표의 형태)으로 input값들이 들어온다고 했을 때 행 단위로 feature마다 가중치가 적용된 후 합하는 과정과 비선형 변환 과정(여기서 활성화 함수가 사용됨)이 모든 행에서 다 이뤄졌을 때 loss function을 이용하여 오차의 합의 평균(평균제곱오차)를 구하게 됨
당연히 결과는 숫자 한 개임 
(`학습할 때마다 변해가는 오차의 합의 평균과 가중치의 변화를 그래프로 그린 것이 손실함수인 것')
--> 근데 여기서 행은 가로줄 모양이고  node(각 input에 대하여 가중치가 적용되는 칸들)은 세로줄 형태로 늘어서 있기 때문에 행렬곱 연산이 이뤄지게 되는 것임!

- 단, dataSet이 너무 크기가 클 경우에는 미니배치(mini batch)라고 하여 작은 단위의 dataSet을 만들어서 학습하게 됨  --> 한번 학습 할 때마다 가중치를 조절하게 됨


**loss function을 통해 계산된 오차의 합의 평균(loss)와 가중치에 대한 그래프**
![](Pasted%20image%2020230706102053.png)
- 가로축이 가중치, 세로축이 오차(에러)를 나타내는 것
- 임의로 가중치를 계속 늘리거나 줄였을 때의 변화를 함수로 나타내면 이런 모양일 것임(이것을 가정하고 출발)

- 오차의 합(평균제곱오차)를 구했다면 이 도함수에서 오차의 최솟값은?
- 당연히 미분값이 0이 되는 지점이 가장 오차가 적은 지점이 될 것임
- 그래서 loss function을 통해 나온 결과값을 미분한 값이 0이 아니라면 학습률(어느 정도나 조정할 것인가에 대한 숫자)을 이용하여 가중치를 조절하게 됨

실제 가중치 조절 공식(얘가 GD라는 옵티마이저)
![](Pasted%20image%2020230706103659.png)
- 해당 지점에서의 기울기 = 미분값
- 학습률은 얼마나 큰 보폭으로 가중치를 조정해나갈지의 대한 숫자
- 막상 공식에 나와 있는 해당 지점에서의 기울기에 위치한 부분에 대입한 결과는 다시 구해야 하는 수식의 형태임(그럼 얘는 또 어케 구함?) 
- 편미분과 연쇄 법칙이 사용됨
![](Pasted%20image%2020230706184404.png)
1. 변수 갯수만큼 각각 편미분함
2. 연쇄 법칙을 활용한 결과 도출(밑에 있는 것이 연쇄 법칙 과정이 있는 식)
![](Pasted%20image%2020230706184519.png)

- 위 공식을 통해 실제 조정할 값을 정하여 갱신이 이뤄진다.
- 이런 식으로 가중치 조절이 이뤄지는 게 바로 경사하강법(GD : Gradient Descent)
- 가중치가 업데이트 되면서 1번 학습된 데이터는 다시 처음으로 돌아가게 됨(학습을 멈추기 전까지 계속 되돌아감)
--> 이래서 Backward Propagation 이라고 부르는 것임


### 옵티마이저 - 경사하강법(GD) 너무 비효율적 ㅋ 해서 나온 알고리즘
SGD - 한 학습단위에 대해서 하나의 데이터만 가지고 학습 but 정확도 떨어짐 
활성화 함수 - 각 학습단위마다 비선형 변환을 하기 위해 1번, 미분할 때 1번 사용된다고 함(자세한 건 모름)








