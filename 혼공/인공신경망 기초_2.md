
**퍼셉트론** : 인공신경망의 구성요소로서 다수의 값을 입력받아 각각 가중치를 곱하고 하나의 값으로 합친 후 계단함수와 같은 활성화함수를 통해 값을 출력하는 프랑크 로젠블라트(Frank Rosenblatt)가 1957년에 제안한 초기 형태의 인공 신경망의 모양
- 퍼셉트론은 이진분류기임 즉, 두 가지 카테고리로 분류하는 모델
- 퍼셉트론 하나만 있을 때 단층 퍼셉트론이라고 부름
- 결국 인공신경망이라는 것도 수학적으로 봤을 땐 수식이므로 알고리즘이라고 표현할 수도 있는 것

결국 최적의 가중치를 찾는 것이 학습의 최종 목표
1. 처음에는 랜덤하게 아무 값이나 가중치로 넣고 학습을 시작함
2. 얼마나 잘 예측했냐는 loss function을 가지고 판단
3. loss를 w로 미분(즉, 오차제곱의 합을 미분)하여 어느 정도로 조정을 할지에 대한 값을 정할 때 사용함 -> 미분값이 포함된 공식을 통해서 조정할 값을 정하고 가중치를 조정하게 됨(경사하강법)

근데 이렇게 한번의 가중치 조정을 위해서 모든 데이터에 대하여 loss를 계산하는 건 너무 비효율적이지 않음?? 물론 모든 데이터를 다 계산해야 하는 만큼 느리기도 하겠지
--> 그래서 전체를 작은 단위로 쪼개서 그 작은 뭉치 단위로 계산하기 시작함(이게 미니배치 학습임)

- Batch Gradient Descent : 모든 데이터를 때려박아가며 학습시키는 방법(위에 말했듯이 비효율적)
Ex) 내가 10억개의 데이터를 가지고 있다면 한 번 가중치를 조정할 때마다 10억개의 데이터를 모두 계산
- Stochastic Gradient Descent(SGD) : 전체 중 1개의 데이터만 뽑아서 학습시키는 방법(부정확할 수 있음)
- 배치 사이즈를 2배 키우면 learning rate도 2배로 키워줘야 된다고 함/ 일반적으로 배치 사이즈가 크면 학습이 더 빨리 되고 성능면에서 더 좋은 편이라고 함


### 미분을 구하는 방법
- Regression 문제를 풀 때 마지막 hidden layer에는 activation function을 적용하지 않는다고 함
- 어쨋든 합산이 다 되고 weight가 적용된 값은 활성화 함수로 비선형 변환이 일어남
1. loss에 대한 함수 미분

loss함수부터 시작해서 거꾸로 한 단계씩 미분값을 구하고 이들을 곱(chain rule)하여 weight에 적용할 값을 구하여 갱신을 진행하므로 back propagation 이라고 함

