
**퍼셉트론** : 인공신경망의 구성요소로서 다수의 값을 입력받아 각각 가중치를 곱하고 하나의 값으로 합친 후 계단함수와 같은 활성화함수를 통해 값을 출력하는 프랑크 로젠블라트(Frank Rosenblatt)가 1957년에 제안한 초기 형태의 인공 신경망의 모양
- 퍼셉트론은 이진분류기임 즉, 두 가지 카테고리로 분류하는 모델
- 퍼셉트론 하나만 있을 때 단층 퍼셉트론이라고 부름
- 결국 인공신경망이라는 것도 수학적으로 봤을 땐 수식이므로 알고리즘이라고 표현할 수도 있는 것

결국 최적의 가중치를 찾는 것이 학습의 최종 목표
1. 처음에는 랜덤하게 아무 값이나 가중치로 넣고 학습을 시작함
2. 얼마나 잘 예측했냐는 loss function을 가지고 판단
3. loss(오차)를 w와 b로 각각 편미분 + 연쇄법칙(합성함수의 미분)
(즉, 오차제곱의 합을 미분/가중치가 여러개일 경우 각각을 편미분)
4. 이렇게 구한 미분값을 어느 정도로 조정을 할지에 대한 값을 정할 때 사용함 -> 미분값이 포함된 공식을 통해서 조정할 값을 정하고 가중치를 조정하게 됨(경사하강법)

근데 이렇게 한번의 가중치 조정을 위해서 모든 데이터에 대하여 loss를 계산하는 건 너무 비효율적이지 않음?? 물론 모든 데이터를 다 계산해야 하는 만큼 느리기도 하겠지

--> 그래서 데이터를 더 작은 뭉치로 쪼개서 학습시키기 시작함(이게 미니배치 학습임)
- Batch Gradient Descent : 모든 데이터를 때려박아가며 학습시키는 방법
- Stochastic Gradient Descent(SGD) : 전체 중 1개의 데이터만 뽑아서 학습시키는 방법

--> 그래서 전체를 작은 단위로 쪼개서 그 작은 뭉치 단위로 계산하기 시작함(이게 미니배치 학습임)

- Batch Gradient Descent : 모든 데이터를 때려박아가며 학습시키는 방법(위에 말했듯이 비효율적)
Ex) 내가 10억개의 데이터를 가지고 있다면 한 번 가중치를 조정할 때마다 10억개의 데이터를 모두 계산
- Stochastic Gradient Descent(SGD) : 전체 중 1개의 데이터만 뽑아서 학습시키는 방법(부정확할 수 있음)
- 배치 사이즈를 2배 키우면 learning rate도 2배로 키워줘야 된다고 함/ 일반적으로 배치 사이즈가 크면 학습이 더 빨리 되고 성능면에서 더 좋은 편이라고 함


### 미분을 구하는 방법
- Regression 문제를 풀 때 마지막 hidden layer에는 activation function을 적용하지 않는다고 함
- 어쨋든 합산이 다 되고 weight가 적용된 값은 활성화 함수로 비선형 변환이 일어남
- **이후 편미분과 연쇄법칙을 이용하여 실제 기울기(미분값)을 구한 후 옵티마이저(공식)를 이용하여 실제 조정을 할 값을 구함(여러 가중치가 있을 경우 여러 가중치는 실제로 동시에 업데이트 된다고 함..)**
loss함수부터 시작해서 거꾸로 한 단계씩 이전 과정들의 결과를 미분한 값들을 구하고 이들을 곱(chain rule)하여 weight에 적용할 값을 구하여 갱신을 진행하므로 back propagation 이라고 함

시그모이드를 사용한 분류 모델은 예측값을 0~1사이의 값 즉, 확률로 출력함
## Tensorflow

- 2015년부터 Release되기 시작함
- 산업에서 많이 사용한다고 함 /파이토치는 주로 연구목적
- 내부코어는 C++로 구성 / 특정 파이썬API에서 요청하면 C++가 실행되는 구조임
- 처음에는 Define and Run 방식이었음(그래프 만드는 부분하고 실행시키는 부분이 분리되어 있는 형태) -> 그래프 모드라고도 함

그래프란? 수식을 노드와 연결지점으로 구성된 형태로 바꾸는 것
- tensorflow에서는 수식을 graph로 변환해야 함
- tensorflow에서 수식을 graph로 변환하는 대표적인 이유(장점)
1. GPU와 같은 가속화 장치를 활용하여 더 빠른 실행속도를 낼 수 있음
2. 전체 수식 중 일부분만 실행할 수 있음(a*w + b라는 수식이 있다고 하면 전체를 다 계산하는 것이 아니라 a*w만 할 수도 있다는 얘기)
3. 해당 수식(모델)을 저장하고 배포하기에 용이함

실행은 Session이란 것을 따로 만들어서 실행했다고 함

- 2.0버전부터는 Eager execution으로 바뀜(그래프를 만들지 않고 Session도 안 쓰는 형태)
- Session 대신 Funciton을 사용
- Auto Graph 라는 것을 도입(기존의 그래프 모드를 버림으로 인해 속도가 감소했는데 그 부분을 보보완하기 위해 도입)

**Colaboratory 코드 참고**
